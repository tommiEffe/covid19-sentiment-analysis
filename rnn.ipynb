{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import datetime\n",
    "import spacy\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleaning_tweets import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers=n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        text = text.unsqueeze(1)  # Add dimension for batch_first=True in LSTM\n",
    "        packed_output, (hidden, cell) = self.rnn(text)\n",
    "        hidden = self.dropout(hidden[-1,:,:])  # Take the last layer's hidden state\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the accuracy function\n",
    "def categorical_accuracy(preds, y):\n",
    "    top_pred = preds.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    return correct.float() / y.shape[0]\n",
    "\n",
    "# define the training function\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, labels = batch\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = categorical_accuracy(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# define the evaluation function\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "                text, labels = batch\n",
    "                text, labels = text.to(device), labels.to(device)\n",
    "                predictions = model(text)\n",
    "                loss = criterion(predictions, labels)\n",
    "                acc = categorical_accuracy(predictions, labels)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=True)\n",
    "        recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=True)\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator), precision, recall, f1\n",
    "# define the function to calculate the time elapsed\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "\n",
    "        return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Coronavirus Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tweets datasets\n",
    "covid_train = pd.read_csv('tweets dataset/Coronavirus tweets NLP - Text Classification/tweets_train.csv', encoding=\"ISO-8859-1\")\n",
    "covid_test = pd.read_csv('tweets dataset/Coronavirus tweets NLP - Text Classification/tweets_test.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_new_train = []\n",
    "for t in covid_train.OriginalTweet:\n",
    "    texts_new_train.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(emoji.demojize(t))))))\n",
    "\n",
    "# Add the cleaned tweets to the dataframe\n",
    "covid_train['cleaned_tweets'] = texts_new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_new_test = []\n",
    "for t in covid_test.OriginalTweet:\n",
    "    texts_new_test.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(emoji.demojize(t))))))\n",
    "\n",
    "# Add the cleaned tweets to the dataframe\n",
    "covid_test['cleaned_tweets'] = texts_new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_train['Sentiment'] = covid_train['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})\n",
    "covid_test['Sentiment'] = covid_test['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=25000)\n",
    "tfidf_vectors_train = tfidf_vectorizer.fit_transform(covid_train['cleaned_tweets']).toarray()\n",
    "tfidf_vectors_test = tfidf_vectorizer.transform(covid_test['cleaned_tweets']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "covid_train['Sentiment'] = label_encoder.fit_transform(covid_train['Sentiment'])\n",
    "covid_test['Sentiment'] = label_encoder.transform(covid_test['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "covid_X_train, covid_X_valid, covid_y_train, covid_y_valid = train_test_split(tfidf_vectors_train, covid_train['Sentiment'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "covid_X_test = tfidf_vectors_test\n",
    "covid_y_test = covid_test['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "covid_X_train = torch.tensor(covid_X_train, dtype=torch.float32)\n",
    "covid_X_valid = torch.tensor(covid_X_valid, dtype=torch.float32)\n",
    "covid_X_test = torch.tensor(covid_X_test, dtype=torch.float32)\n",
    "covid_y_train = torch.tensor(covid_y_train, dtype=torch.long)\n",
    "covid_y_valid = torch.tensor(covid_y_valid, dtype=torch.long)\n",
    "covid_y_test = torch.tensor(covid_y_test, dtype=torch.long)\n",
    "\n",
    "# Move tensors to the GPU if available\n",
    "covid_X_train = covid_X_train.to(device)\n",
    "covid_X_valid = covid_X_valid.to(device)\n",
    "covid_X_test = covid_X_test.to(device)\n",
    "covid_y_train = covid_y_train.to(device)\n",
    "covid_y_valid = covid_y_valid.to(device)\n",
    "covid_y_test = covid_y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "covid_train_data = torch.utils.data.TensorDataset(covid_X_train, covid_y_train)\n",
    "covid_valid_data = torch.utils.data.TensorDataset(covid_X_valid, covid_y_valid)\n",
    "covid_test_data = torch.utils.data.TensorDataset(covid_X_test, covid_y_test)\n",
    "\n",
    "covid_train_iterator = torch.utils.data.DataLoader(covid_train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "covid_valid_iterator = torch.utils.data.DataLoader(covid_valid_data, batch_size=BATCH_SIZE)\n",
    "covid_test_iterator = torch.utils.data.DataLoader(covid_test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "INPUT_DIM = covid_X_train.shape[1]\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 3  # Six classes\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = RNN(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Move criterion to GPU\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 16m 25s\n",
      "\tTrain Loss: 0.715 | Train Acc: 68.66%\n",
      "\t Val. Loss: 0.538 |  Val. Acc: 79.38%\n",
      "Epoch: 02 | Epoch Time: 15m 51s\n",
      "\tTrain Loss: 0.359 | Train Acc: 87.36%\n",
      "\t Val. Loss: 0.515 |  Val. Acc: 80.74%\n",
      "Epoch: 03 | Epoch Time: 16m 29s\n",
      "\tTrain Loss: 0.230 | Train Acc: 92.29%\n",
      "\t Val. Loss: 0.569 |  Val. Acc: 80.26%\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "N_EPOCHS = 3\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, covid_train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc, _, _, _ = evaluate(model, covid_valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.592 | Test Acc: 78.97%\n",
      "Test Precision: 1.000 | Test Recall: 1.000 | Test F1: 0.000\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "\n",
    "test_loss, test_acc, test_precision, test_recall, test_f1 = evaluate(model, covid_test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "print(f'Test Precision: {test_precision:.3f} | Test Recall: {test_recall:.3f} | Test F1: {test_f1:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
