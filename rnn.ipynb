{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import datetime\n",
    "import spacy\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleaning_tweets import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers=n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        text = text.unsqueeze(1)  # Add dimension for batch_first=True in LSTM\n",
    "        packed_output, (hidden, cell) = self.rnn(text)\n",
    "        hidden = self.dropout(hidden[-1,:,:])  # Take the last layer's hidden state\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the accuracy function\n",
    "def categorical_accuracy(preds, y):\n",
    "    top_pred = preds.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    return correct.float() / y.shape[0]\n",
    "\n",
    "# define the training function\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, labels = batch\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = categorical_accuracy(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# define the evaluation function\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "                text, labels = batch\n",
    "                text, labels = text.to(device), labels.to(device)\n",
    "                predictions = model(text)\n",
    "                loss = criterion(predictions, labels)\n",
    "                acc = categorical_accuracy(predictions, labels)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "        precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=True)\n",
    "        recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=True)\n",
    "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator), precision, recall, f1\n",
    "# define the function to calculate the time elapsed\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "\n",
    "        return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Coronavirus Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tweets datasets\n",
    "covid_train = pd.read_csv('tweets dataset/Coronavirus tweets NLP - Text Classification/tweets_train.csv', encoding=\"ISO-8859-1\")\n",
    "covid_test = pd.read_csv('tweets dataset/Coronavirus tweets NLP - Text Classification/tweets_test.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_new_train = []\n",
    "for t in covid_train.OriginalTweet:\n",
    "    texts_new_train.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(emoji.demojize(t))))))\n",
    "\n",
    "# Add the cleaned tweets to the dataframe\n",
    "covid_train['cleaned_tweets'] = texts_new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_new_test = []\n",
    "for t in covid_test.OriginalTweet:\n",
    "    texts_new_test.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(emoji.demojize(t))))))\n",
    "\n",
    "# Add the cleaned tweets to the dataframe\n",
    "covid_test['cleaned_tweets'] = texts_new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_train['Sentiment'] = covid_train['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})\n",
    "covid_test['Sentiment'] = covid_test['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=25000)\n",
    "tfidf_vectors_train = tfidf_vectorizer.fit_transform(covid_train['cleaned_tweets']).toarray()\n",
    "tfidf_vectors_test = tfidf_vectorizer.transform(covid_test['cleaned_tweets']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "covid_train['Sentiment'] = label_encoder.fit_transform(covid_train['Sentiment'])\n",
    "covid_test['Sentiment'] = label_encoder.transform(covid_test['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "covid_X_train, covid_X_valid, covid_y_train, covid_y_valid = train_test_split(tfidf_vectors_train, covid_train['Sentiment'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "covid_X_test = tfidf_vectors_test\n",
    "covid_y_test = covid_test['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "covid_X_train = torch.tensor(covid_X_train, dtype=torch.float32)\n",
    "covid_X_valid = torch.tensor(covid_X_valid, dtype=torch.float32)\n",
    "covid_X_test = torch.tensor(covid_X_test, dtype=torch.float32)\n",
    "covid_y_train = torch.tensor(covid_y_train, dtype=torch.long)\n",
    "covid_y_valid = torch.tensor(covid_y_valid, dtype=torch.long)\n",
    "covid_y_test = torch.tensor(covid_y_test, dtype=torch.long)\n",
    "\n",
    "# Move tensors to the GPU if available\n",
    "covid_X_train = covid_X_train.to(device)\n",
    "covid_X_valid = covid_X_valid.to(device)\n",
    "covid_X_test = covid_X_test.to(device)\n",
    "covid_y_train = covid_y_train.to(device)\n",
    "covid_y_valid = covid_y_valid.to(device)\n",
    "covid_y_test = covid_y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "covid_train_data = torch.utils.data.TensorDataset(covid_X_train, covid_y_train)\n",
    "covid_valid_data = torch.utils.data.TensorDataset(covid_X_valid, covid_y_valid)\n",
    "covid_test_data = torch.utils.data.TensorDataset(covid_X_test, covid_y_test)\n",
    "\n",
    "covid_train_iterator = torch.utils.data.DataLoader(covid_train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "covid_valid_iterator = torch.utils.data.DataLoader(covid_valid_data, batch_size=BATCH_SIZE)\n",
    "covid_test_iterator = torch.utils.data.DataLoader(covid_test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "INPUT_DIM = covid_X_train.shape[1]\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 3  # Six classes\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = RNN(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Move criterion to GPU\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m      9\u001b[0m         start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 11\u001b[0m         train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovid_train_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m         valid_loss, valid_acc, _, _, _ \u001b[38;5;241m=\u001b[39m evaluate(model, covid_valid_iterator, criterion)\n\u001b[0;32m     14\u001b[0m         end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[0;32m     22\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 24\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     epoch_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(iterator), epoch_acc \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(iterator)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "\n",
    "N_EPOCHS = 3\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, covid_train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc, _, _, _ = evaluate(model, covid_valid_iterator, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'liar-model.pt')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m test_loss, test_acc, test_precision, test_recall, test_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovid_test_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_precision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_recall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 48\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, iterator, criterion)\u001b[0m\n\u001b[0;32m     46\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(predictions, labels)\n\u001b[0;32m     47\u001b[0m         acc \u001b[38;5;241m=\u001b[39m categorical_accuracy(predictions, labels)\n\u001b[1;32m---> 48\u001b[0m         epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m         epoch_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     51\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(all_labels, all_predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "\n",
    "test_loss, test_acc, test_precision, test_recall, test_f1 = evaluate(model, covid_test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n",
    "print(f'Test Precision: {test_precision:.3f} | Test Recall: {test_recall:.3f} | Test F1: {test_f1:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
