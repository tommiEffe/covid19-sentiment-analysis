{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleaning_tweets import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coronavirus Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tweets datasets\n",
    "coronavirus_tweets_nlp_train = pd.read_csv('tweets dataset/Coronavirus tweets NLP - Text Classification/tweets_train.csv', encoding=\"ISO-8859-1\")\n",
    "coronavirus_tweets_nlp_test = pd.read_csv('tweets dataset/Coronavirus tweets NLP - Text Classification/tweets_test.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserName            0\n",
      "ScreenName          0\n",
      "Location         8590\n",
      "TweetAt             0\n",
      "OriginalTweet       0\n",
      "Sentiment           0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "UserName           0\n",
      "ScreenName         0\n",
      "Location         834\n",
      "TweetAt            0\n",
      "OriginalTweet      0\n",
      "Sentiment          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for missing values in text and label columns\n",
    "print(coronavirus_tweets_nlp_train.isnull().sum())\n",
    "print('\\n')\n",
    "print(coronavirus_tweets_nlp_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_new_train = []\n",
    "for t in coronavirus_tweets_nlp_train.OriginalTweet:\n",
    "    texts_new_train.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(emoji.demojize(t))))))\n",
    "\n",
    "# Add the cleaned tweets to the dataframe\n",
    "coronavirus_tweets_nlp_train['cleaned_tweets'] = texts_new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_new_test = []\n",
    "for t in coronavirus_tweets_nlp_test.OriginalTweet:\n",
    "    texts_new_test.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(emoji.demojize(t))))))\n",
    "\n",
    "# Add the cleaned tweets to the dataframe\n",
    "coronavirus_tweets_nlp_test['cleaned_tweets'] = texts_new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "coronavirus_tweets_nlp_train['Sentiment'] = coronavirus_tweets_nlp_train['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})\n",
    "coronavirus_tweets_nlp_test['Sentiment'] = coronavirus_tweets_nlp_test['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/charlesmalafosse/open-dataset-for-sentiment-analysis/blob/62425b270bcf7561b7a6f7821a09f5bf522a798f/betsentiment-IT-tweets-sentiment-teams-split.zip.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_df = pd.read_csv('tweets dataset/Open IT Dataset for Sentiment Analysis/betsentiment-IT-tweets-sentiment-players.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_date_created    0\n",
      "tweet_id              0\n",
      "tweet_text            0\n",
      "language              0\n",
      "sentiment             0\n",
      "sentiment_score       0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for missing values in text and label columns\n",
    "print(it_df.isnull().sum())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_new_train = []\n",
    "for t in it_df.tweet_text:\n",
    "    texts_new_train.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(emoji.demojize(t))))))\n",
    "\n",
    "# Add the cleaned tweets to the dataframe\n",
    "it_df['cleaned_tweets'] = texts_new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_df['sentiment'] = it_df['sentiment'].map({'NEGATIVE':0,'NEUTRAL':1,'MIXED':1,'POSITIVE':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the df into train and test\n",
    "it_df_train, it_df_test = train_test_split(it_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/charunisa/chatgpt-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_df = pd.read_csv('tweets dataset/ChatGPT sentiment analysis/file.csv')\n",
    "chatgpt_df = chatgpt_df[['tweets', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets    0\n",
      "labels    0\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check for missing values in text and label columns\n",
    "print(chatgpt_df.isnull().sum())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_new_train = []\n",
    "for t in chatgpt_df.tweets:\n",
    "    texts_new_train.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(emoji.demojize(t))))))\n",
    "\n",
    "# Add the cleaned tweets to the dataframe\n",
    "chatgpt_df['cleaned_tweets'] = texts_new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatgpt_df['labels'] = chatgpt_df['labels'].map({'bad':0,'neutral':1,'good':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the df into train and test\n",
    "chatgpt_df_train, chatgpt_df_test = train_test_split(chatgpt_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in the Covid19 dataset: 41157\n",
      "Number of tweets in the Covid19 test dataset: 3798\n",
      "Number of tweets in each class in the Covid19 training dataset:\n",
      "2    43.8%\n",
      "0    37.4%\n",
      "1    18.7%\n",
      "Name: Sentiment, dtype: object\n",
      "\n",
      "\n",
      "Number of tweets in the Italian dataset: 132652\n",
      "Number of tweets in the Italian test dataset: 33163\n",
      "Number of tweets in each class in the Italian training set:\n",
      "1    81.9%\n",
      "2    14.2%\n",
      "0     3.9%\n",
      "Name: sentiment, dtype: object\n",
      "\n",
      "\n",
      "Number of tweets in the ChatGPT dataset: 175435\n",
      "Number of tweets in the ChatGPT test dataset: 43859\n",
      "Number of tweets in each class in the ChatGPT training set:\n",
      "0    49.2%\n",
      "2    25.5%\n",
      "1    25.3%\n",
      "Name: labels, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# print total number of tweets in each dataset\n",
    "print('Number of tweets in the Covid19 dataset:', len(coronavirus_tweets_nlp_train))\n",
    "print('Number of tweets in the Covid19 test dataset:', len(coronavirus_tweets_nlp_test))\n",
    "print('Number of tweets in each class in the Covid19 training dataset:')\n",
    "print(coronavirus_tweets_nlp_train['Sentiment'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%')\n",
    "print('\\n')\n",
    "print('Number of tweets in the Italian dataset:', len(it_df_train))\n",
    "print('Number of tweets in the Italian test dataset:', len(it_df_test))\n",
    "print('Number of tweets in each class in the Italian training set:')\n",
    "print(it_df_train['sentiment'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%')\n",
    "print('\\n')\n",
    "print('Number of tweets in the ChatGPT dataset:', len(chatgpt_df_train))\n",
    "print('Number of tweets in the ChatGPT test dataset:', len(chatgpt_df_test))\n",
    "print('Number of tweets in each class in the ChatGPT training set:')\n",
    "print(chatgpt_df_train['labels'].value_counts(normalize=True).mul(100).round(1).astype(str) + '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PassiveAggressive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coronavirus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x_train, y_train, x_test, y_test\n",
    "coronavirus_tweets_nlp_x_train = coronavirus_tweets_nlp_train['cleaned_tweets']\n",
    "coronavirus_tweets_nlp_y_train = coronavirus_tweets_nlp_train['Sentiment']\n",
    "coronavirus_tweets_nlp_x_test = coronavirus_tweets_nlp_test['cleaned_tweets']\n",
    "coronavirus_tweets_nlp_y_test = coronavirus_tweets_nlp_test['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['0', '1', '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9)\n",
    "\n",
    "# fit and transform train set, transform test set\n",
    "coronavirus_tweets_nlp_tfidf_train = tfidf_vectorizer.fit_transform(coronavirus_tweets_nlp_x_train)\n",
    "coronavirus_tweets_nlp_tfidf_test = tfidf_vectorizer.transform(coronavirus_tweets_nlp_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n",
      "Precision: 0.74\n",
      "Recall: 0.74\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1219  164  250]\n",
      " [ 146  356  117]\n",
      " [ 206  101 1239]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomma\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# initialize a PassiveAggressiveClassifier and fit the model\n",
    "pac = PassiveAggressiveClassifier(max_iter=50)\n",
    "pac.fit(coronavirus_tweets_nlp_tfidf_train, coronavirus_tweets_nlp_y_train)\n",
    "y_pred = pac.predict(coronavirus_tweets_nlp_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(coronavirus_tweets_nlp_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(coronavirus_tweets_nlp_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(coronavirus_tweets_nlp_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(coronavirus_tweets_nlp_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x_train, y_train, x_test, y_test\n",
    "it_x_train = it_df_train['cleaned_tweets']\n",
    "it_y_train = it_df_train['sentiment']\n",
    "it_x_test = it_df_test['cleaned_tweets']\n",
    "it_y_test = it_df_test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['0', '1', '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9)\n",
    "\n",
    "# fit and transform train set, transform test set\n",
    "it_tfidf_train = tfidf_vectorizer.fit_transform(it_x_train)\n",
    "it_tfidf_test = tfidf_vectorizer.transform(it_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "Precision: 0.81\n",
      "Recall: 0.82\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  350   877    93]\n",
      " [  541 24441  2142]\n",
      " [   71  2303  2345]]\n"
     ]
    }
   ],
   "source": [
    "# initialize a PassiveAggressiveClassifier and fit the model\n",
    "pac = PassiveAggressiveClassifier(max_iter=50)\n",
    "pac.fit(it_tfidf_train, it_y_train)\n",
    "y_pred = pac.predict(it_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(it_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(it_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(it_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(it_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x_train, y_train, x_test, y_test\n",
    "chatgpt_x_train = chatgpt_df_train['cleaned_tweets']\n",
    "chatgpt_y_train = chatgpt_df_train['labels']\n",
    "chatgpt_x_test = chatgpt_df_test['cleaned_tweets']\n",
    "chatgpt_y_test = chatgpt_df_test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['0', '1', '2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9)\n",
    "\n",
    "# fit and transform train set, transform test set\n",
    "chatgpt_tfidf_train = tfidf_vectorizer.fit_transform(chatgpt_x_train)\n",
    "chatgpt_tfidf_test = tfidf_vectorizer.transform(chatgpt_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n",
      "Precision: 0.80\n",
      "Recall: 0.80\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19671  1543   260]\n",
      " [ 2576  6776  1829]\n",
      " [  433  2036  8735]]\n"
     ]
    }
   ],
   "source": [
    "# initialize a PassiveAggressiveClassifier and fit the model\n",
    "pac = PassiveAggressiveClassifier(max_iter=50)\n",
    "pac.fit(chatgpt_tfidf_train, chatgpt_y_train)\n",
    "y_pred = pac.predict(chatgpt_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(chatgpt_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(chatgpt_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(chatgpt_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(chatgpt_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coronavirus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "Precision: 0.77\n",
      "Recall: 0.77\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1289  100  244]\n",
      " [ 155  353  111]\n",
      " [ 193   60 1293]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomma\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# inizialite logistic regression and fit the model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(coronavirus_tweets_nlp_tfidf_train, coronavirus_tweets_nlp_y_train)\n",
    "y_pred = logreg.predict(coronavirus_tweets_nlp_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(coronavirus_tweets_nlp_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(coronavirus_tweets_nlp_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(coronavirus_tweets_nlp_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(coronavirus_tweets_nlp_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Precision: 0.84\n",
      "Recall: 0.85\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  194  1107    19]\n",
      " [   99 26144   881]\n",
      " [   10  2745  1964]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomma\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# initialize a logistic regression and fit the model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(it_tfidf_train, it_y_train)\n",
    "y_pred = logreg.predict(it_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(it_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(it_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(it_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(it_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n",
      "Precision: 0.82\n",
      "Recall: 0.83\n",
      "\n",
      "Confusion Matrix:\n",
      "[[20364   839   271]\n",
      " [ 2529  6937  1715]\n",
      " [  581  1558  9065]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomma\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# initialize a logistic regression and fit the model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(chatgpt_tfidf_train, chatgpt_y_train)\n",
    "y_pred = logreg.predict(chatgpt_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(chatgpt_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(chatgpt_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(chatgpt_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(chatgpt_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forrest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coronavirus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n",
      "Precision: 0.71\n",
      "Recall: 0.71\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1145  151  337]\n",
      " [ 146  349  124]\n",
      " [ 253  103 1190]]\n"
     ]
    }
   ],
   "source": [
    "# inizialite random forrest and fit the model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(coronavirus_tweets_nlp_tfidf_train, coronavirus_tweets_nlp_y_train)\n",
    "y_pred = rf.predict(coronavirus_tweets_nlp_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(coronavirus_tweets_nlp_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(coronavirus_tweets_nlp_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(coronavirus_tweets_nlp_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(coronavirus_tweets_nlp_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n",
      "Precision: 0.82\n",
      "Recall: 0.84\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   29  1287     4]\n",
      " [   20 26646   458]\n",
      " [    0  3551  1168]]\n"
     ]
    }
   ],
   "source": [
    "#initialize a random forrest and fit the model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(it_tfidf_train, it_y_train)\n",
    "y_pred = rf.predict(it_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(it_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(it_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(it_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(it_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n",
      "Precision: 0.79\n",
      "Recall: 0.80\n",
      "\n",
      "Confusion Matrix:\n",
      "[[19852   973   649]\n",
      " [ 2818  6718  1645]\n",
      " [  870  1991  8343]]\n"
     ]
    }
   ],
   "source": [
    "#initialize a random forrest and fit the model\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(chatgpt_tfidf_train, chatgpt_y_train)\n",
    "y_pred = rf.predict(chatgpt_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(chatgpt_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(chatgpt_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(chatgpt_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(chatgpt_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Support Vector Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coronavirus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n",
      "Precision: 0.78\n",
      "Recall: 0.78\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1302  117  214]\n",
      " [ 143  380   96]\n",
      " [ 186   64 1296]]\n"
     ]
    }
   ],
   "source": [
    "# inizialite svc and fit the model\n",
    "svc = LinearSVC()\n",
    "svc.fit(coronavirus_tweets_nlp_tfidf_train, coronavirus_tweets_nlp_y_train)\n",
    "y_pred = svc.predict(coronavirus_tweets_nlp_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(coronavirus_tweets_nlp_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(coronavirus_tweets_nlp_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(coronavirus_tweets_nlp_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(coronavirus_tweets_nlp_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Precision: 0.83\n",
      "Recall: 0.85\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  234  1044    42]\n",
      " [  141 25899  1084]\n",
      " [   14  2612  2093]]\n"
     ]
    }
   ],
   "source": [
    "#initialize svc and fit the model\n",
    "svc = LinearSVC()\n",
    "svc.fit(it_tfidf_train, it_y_train)\n",
    "y_pred = svc.predict(it_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(it_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(it_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(it_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(it_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "Precision: 0.82\n",
      "Recall: 0.82\n",
      "\n",
      "Confusion Matrix:\n",
      "[[20477   692   305]\n",
      " [ 2609  6223  2349]\n",
      " [  410  1329  9465]]\n"
     ]
    }
   ],
   "source": [
    "#initialize svc and fit the model\n",
    "svc = LinearSVC()\n",
    "svc.fit(chatgpt_tfidf_train, chatgpt_y_train)\n",
    "y_pred = svc.predict(chatgpt_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(chatgpt_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(chatgpt_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(chatgpt_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(chatgpt_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coronavirus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.73\n",
      "Precision: 0.74\n",
      "Recall: 0.73\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1150  185  298]\n",
      " [  95  430   94]\n",
      " [ 219  125 1202]]\n"
     ]
    }
   ],
   "source": [
    "# inizialite XGB and fit the model\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(coronavirus_tweets_nlp_tfidf_train, coronavirus_tweets_nlp_y_train)\n",
    "y_pred = xgb.predict(coronavirus_tweets_nlp_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(coronavirus_tweets_nlp_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(coronavirus_tweets_nlp_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(coronavirus_tweets_nlp_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(coronavirus_tweets_nlp_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Precision: 0.83\n",
      "Recall: 0.85\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  124  1180    16]\n",
      " [   87 26341   696]\n",
      " [   13  2997  1709]]\n"
     ]
    }
   ],
   "source": [
    "# initialize XGB and fit the model\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(it_tfidf_train, it_y_train)\n",
    "y_pred = xgb.predict(it_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(it_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(it_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(it_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(it_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n",
      "Precision: 0.73\n",
      "Recall: 0.74\n",
      "\n",
      "Confusion Matrix:\n",
      "[[20188   693   593]\n",
      " [ 4947  4794  1440]\n",
      " [ 1945  1795  7464]]\n"
     ]
    }
   ],
   "source": [
    "# initialize XGB and fit the model\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(chatgpt_tfidf_train, chatgpt_y_train)\n",
    "y_pred = xgb.predict(chatgpt_tfidf_test)\n",
    "\n",
    "# print the classification report\n",
    "print(f'Accuracy: {accuracy_score(chatgpt_y_test, y_pred):.2f}')\n",
    "print(f'Precision: {precision_score(chatgpt_y_test, y_pred, average=\"weighted\"):.2f}')\n",
    "print(f'Recall: {recall_score(chatgpt_y_test, y_pred, average=\"weighted\"):.2f}\\n')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(chatgpt_y_test, y_pred, labels=[0, 1, 2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
